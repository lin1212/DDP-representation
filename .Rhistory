install.packages("ggplot2")
library(ggplot2)
qplot(Wind,Ozone, data=airquality, facets=.~factor(Month))
airquality=transform(airquality, Month=factor(Month))
qplot(Wind,Ozone, data=airquality, facets=.~Month)
qplot(votes,rating, data=movies, smooth="loess")
qplot(votes,rating, data=movies) +geom_smooth()
install.packages("knitr")
library(lattic)
library(lattice)
weekday
download.file("http://d396qusza40orc.cloudfront.net/repdata%2Fdata%2Factivity.zip","activity.zip")
unzip("activity.zip")
activity <- read.csv("activity.csv", as.is = T)
activity$date <- as.Date(activity$date, "%Y-%m-%d")
imputed_steps = ifelse(is.na(activity$steps), avg$steps[match(activity$interval, avg$interval)], activity$steps)
imputed_data <- transform(activity, steps = imputed_steps)
avg <- aggregate(steps ~ interval, data = activity, mean, na.rm = TRUE)
imputed_steps = ifelse(is.na(activity$steps), avg$steps[match(activity$interval, avg$interval)], activity$steps)
imputed_data <- transform(activity, steps = imputed_steps)
MonToFri <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")
weekday <- as.factor(ifelse(is.element(weekdays(imputed_data$date), MonToFri), "weekday", "weekend"))
weekday
imputed_data$date
weekdays(imputed_data$date)
language=en
?language
Rscript
weekdays(imputed_data$date)
weekdays(imputed_data$date)
download.file("http://d396qusza40orc.cloudfront.net/repdata%2Fdata%2Factivity.zip","activity.zip")
unzip("activity.zip")
activity <- read.csv("activity.csv", as.is = T)
activity$date <- as.Date(activity$date, "%Y-%m-%d")
avg <- aggregate(steps ~ interval, data = activity, mean, na.rm = TRUE)
imputed_steps = ifelse(is.na(activity$steps), avg$steps[match(activity$interval, avg$interval)], activity$steps)
imputed_data <- transform(activity, steps = imputed_steps)
MonToFri <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")
weekday <- as.factor(ifelse(is.element(weekdays(imputed_data$date), MonToFri), "weekday", "weekend"))
weekdays(imputed_data$date)
weekdays(imputed_data$date)
today()
?today
?date
date()
weekday(date(0))
weekday(date())
weekdays(date())
weekdays(as.Date(date())
)
tmp= as.Date("2014-09-12", "%Y-%m-%d")
tmp
weekdays(tmp)
?pbinom
pbinom(4,1,0.5)
dbinom(4,1,0.5)
dbinom(4,5,0.5)
dbinom(5,5,0.5)
dbinom(5,5,0.5)+dbinom(4,5,0.5)
6/32
?ppois
ppois(10, 15)
ppois(15, 15)
qt(.95, 8)
qt(.05, 8)
1100+qt(.95,8) * 30/sqrt(9)
1100-qt(.95,8) * 30/sqrt(9)
1100-qt(.975,8) * 30/sqrt(9)
1100+qt(.975,8) * 30/sqrt(9)
6/qt(.975,8)
?relevel
?t.test
library(reshape2)
?dcast
?melt
names(airquality) <- tolower(names(airquality))
head(airquality)
aqm <- melt(airquality, id=c("month", "day"), na.rm=TRUE)
head(aqm)
acast(aqm, day ~ month ~ variable)
aqm
acast(aqm, day ~ month ~ variable)
dcast(aqm, month ~ variable, mean, margins = c("month", "variable"))
2*sqrt(10/(2*.6*.68))
t=-2*sqrt(10/(0.6+0.68))
t
t+ qt(.975,9)*sqrt(10/(.6+.68))
t- qt(.975,9)*sqrt(10/(.6+.68))
-2- qt(.975,9)*sqrt(10/(.6+.68))
-2+ qt(.975,9)*sqrt(10/(.6+.68))
-2+ qt(.975,9)*sqrt((.6+.68)/10)
-2- qt(.975,9)*sqrt((.6+.68)/10)
-2- qt(.975,18)*sqrt((.6+.68)/10)
-2+ qt(.975,18)*sqrt((.6+.68)/10)
qnorm(.975)
2+1.96*sqrt(4.25)/10
2-1.96*sqrt(4.25)/10
-4+qt(.975,16)*sqrt(1.5*1.5+1.8*1.8)/3
-4-qt(.975,16)*sqrt(1.5*1.5+1.8*1.8)/3
-4-qt(.95,16)*sqrt(1.5*1.5+1.8*1.8)/3
-4+qt(.95,16)*sqrt(1.5*1.5+1.8*1.8)/3
-8-3+1-2-5
sd(1,2)
std(1,2)
?sd
sd(1,2,3)
a= c(-8,-3,1,-2,-5)
mean(a)
sd(a)
t=mean(a)/sd(d)*sqrt(5)
t=mean(a)/sd(a)*sqrt(5)
t
pt(t, 4)
?pt
qnorm(.95)
pnorm(10+qnorm(.95)*.4, mean=11, .4)
1-pnorm(10+qnorm(.95)*.4, mean=11, .4)
pbinom(55, .5,100)
?pbinom
pbinom(54, .5,100)
pbinom(54, 0.5, 100)
pbinom(54, prob=0.5, size=100)
pbinom(55, prob=0.5, size=100)
1-pbinom(55, prob=0.5, size=100)
1-pbinom(54, prob=0.5, size=100)
pbinom(2, prob=0.5, size=4)
pbinom(2, prob=0.5, size=4, lower.tail=F)
pbinom(3, prob=0.5, size=4, lower.tail=F)
pbinom(10, prob=0.01, size=1787)
t = -4/sqrt(1.5*1.5+1.8*1.8)/3
t
pt(t, 16)
pnorm(.004*qnorm(.95), mean = 0.01, sd = 0.04)
1-pnorm(.004*qnorm(.95), mean = 0.01, sd = 0.04)
(qnorm(.9)+qnorm(.95))^2*.04^2/.01^2
t = -4/(sqrt(1.5*1.5+1.8*1.8)/3)
t
qt(t, 16)
pt(t, 16)
1-pnorm(.004*qnorm(.95), mean = 0.01, sd = 0.04)
1-pnorm(.004*qnorm(.95) + 0.01, mean = 0, sd = 0.04)
1-pnorm(.004*qnorm(.95), mean = 0.01, sd = 0.004)
download.file("https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2", "storm.bz2")
getwd()
storm <- read.csv(bzfile("storm.bz2"))
storm <- read.csv(bzfile("storm.bz2"))
library(knitr)
par(mfrow=c(1,2), mar = c(12, 4, 3, 2), cex = 0.7)
barplot(deadtop10$FATALITIES, names.arg=deadtop10$EVTYPE, las = 3,
main = "Top 10 harmful events with respect to fatalities",
ylab="Number of fatalities")
barplot(injurytop10$INJURIES, names.arg=injurytop10$EVTYPE, las = 3,
main = "Top 10 harmful events with respect to injuries",
ylab="Number of injuries")
head(storm)
storm <- read.csv(bzfile("storm.bz2"))
sessioninfo()
sessionInfo()
2500/450+2500/300+2500/150+2500/100
?matrix
t<-matrix(c(400, 350, 200,50), nrow=2)
t
t<-matrix(c(400, 200, 350, 50), nrow=2)
t
chisq(t)
chisq.test(t)
2500*(1/450+1/300+1/150+1/100)
2500+750
3250+250
35/40
t<-matrix(c(50, 750, 200, 2500), nrow=2)
t
chisq.test(t)
250*64
40*400
400/5
getwd()
?read.table
web <- read.table("../algs4/PageRank/web-Google.txt", skip=4)
head(web)
dim(web)
names(web) <- c("FromNodeID", "ToNodeID")
head(web)
max(web$FromNodeID)
?unique
unique(web$FromNodeID)
max(web$ToNodeID)
Nodes <- unique(c(web$FromNodeID,web$ToNodeID))
dim(Nodes)
Nodes
c(web$FromNodeID,web$ToNodeID)
str(Nodes)
max(Nodes)
length(NOdes)
length(Nodes)
5105039*2
web <- read.table("../algs4/PageRank/web-Google.txt", skip=4)
names(web) <- c("FromNodeID", "ToNodeID")
head(web)
attach(mtcars)
head(mtcars)
lm(mpg~mt+factor(cyl))
lm(mpg~wt+factor(cyl))
fit<-lm(mpg~wt+factor(cyl))
summary(fit)
fit0<-lm(mpg~factor(cyl))
summary(fit0)
fit2<-lm(mpg~wt*factor(cyl))
summary(fit2)
anova(fit,fit2)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
m<-lm(y~x)
hatvalues(m)
dfbetas(m)
fit$coef
fit3<-lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
fit3$coef
help(mtcars)
install.packages("caret")
library(caret)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData=data.frame(diagnosis,predictors)
inTrain=createDataPartition(adData$diagnosis, p=.75)[[1]]
training=adData[inTrain,]
testing=adData[-inTrain,]
head(training)
dim(training)
names(training)
grepl("IL", names(training))
grepl(^IL, names(training))
grepl("^IL", names(training))
v<-training[,grepl("^IL", names(training))]
head(v)
pca<-preProcess(v,method="pca")
pca
dim(v)
dim(training)
?preProcess
summary(pca)
pca<-preProcess(v,method="pca",pcaComp=10)
pca
pca$pcaComp
summary(pca)
pca$numComp
pca<-preProcess(v,method="pca",thresh=.9)
pca
newtrain<-cbind(training$diagnosis, v)
head(newtrain)
fit1<-train(training$diagnosis~. method="glm", data=v)
confusionMatrix(testing$diagnosis,predict(fit1,testing))
fit1<-train(training$diagnosis~., method="glm", data=v)
confusionMatrix(testing$diagnosis,predict(fit1,testing))
install.packages("e1071")
library(e1071)
fit1<-train(training$diagnosis~., method="glm", data=v)
confusionMatrix(testing$diagnosis,predict(fit1,testing))
fit2<-train(training$diagnosis~., method="glm", preProcess="pca", data=v)
confusionMatrix(testing$diagnosis,predict(fit2,testing))
set.seed(975)
inTrain=createDataPartition(mixtures$CompressiveStrength, p=.75)[[1]]
training=mixtures[inTrain,]
testing=mixtures[-inTrain,]
cutS<-cut2(training$CompressiveStrength, g=4)
plot(training$CompressiveStrength, col=cutS)
install.packages("Hmist")
library(Hmist)
install.packages("Hmisc")
library(Hmisc)
set.seed(975)
inTrain=createDataPartition(mixtures$CompressiveStrength, p=.75)[[1]]
training=mixtures[inTrain,]
testing=mixtures[-inTrain,]
cutS<-cut2(training$CompressiveStrength, g=4)
plot(training$CompressiveStrength, col=cutS)
cutS
cutS<-cut2(training$CompressiveStrength, g=4)
?cut2
training
dim(training)
inTrain=createDataPartition(mixtures$CompressiveStrength, p=.75)[[1]]
library(AppliedPredictiveModeling)
data(mixtures)
data(mixtures)
inTrain=createDataPartition(mixtures$CompressiveStrength, p=.75)[[1]]
data(concrete)
inTrain=createDataPartition(mixtures$CompressiveStrength, p=.75)[[1]]
training=mixtures[inTrain,]
testing=mixtures[-inTrain,]
cutS<-cut2(training$CompressiveStrength, g=4)
plot(training$CompressiveStrength, col=cutS)
qplot(cutS, Age,data=training, fill=cutS, geom=c("boxplot") )
qplot(cutS, FlyAsh,data=training, fill=cutS, geom=c("boxplot") )
qplot(cutS, Age,data=training, fill=cutS )
hist(training$Superplasticizer)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData=data.frame(diagnosis,predictors)
inTrain=createDataPartition(adData$diagnosis, p=.75)[[1]]
training=adData[inTrain,]
testing=adData[-inTrain,]
v<-training[,grepl("^IL", names(training))]
pca<-preProcess(v,method="pca",thresh=.8)
pca
fit2<-train(training$diagnosis~., method="glm", preProcess="pca", thresh=.8, data=v)
confusionMatrix(testing$diagnosis,predict(fit2,testing))
data(vowel.train);data(vowel.test)
install.packages(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(vowel.train);data(vowel.test)
vowel.train$y<-factor(vowel.train$y)
vowel.test$y<-factor(vowel.test$y)
set.seed(33833)
fit1<-train(y~., method="rf",data=vowel.train)
fit1<-train(y~., method="rf",data=vowel.train)
varImp(fit)
varImp(fit1)
set.seed(3833)
fit1<-train(y~., method="rf",data=vowel.train)
varImp(fit1)
fit1<-train(y~., method="rf",data=vowel.train, importance=F)
varImp(fit1)
set.seed(33833)
fit1<-train(y~., method="rf",data=vowel.train, importance=F)
varImp(fit1)
confusionMatrix(pred1,vowel.test$y)$overall;
pred1<-predict(fit1, vowel.test)
confusionMatrix(pred1,vowel.test$y)$overall;
fit2<-train(y~., method="gbm",data=vowel.train, verbose=F)
fit2<-train(y~., method="gbm",data=vowel.train, verbose=F)
pred2<-predict(fit2,vowel.test);
confusionMatrix(pred2,vowel.test$y)$overall
sum(pred1==pred2)
pred1
sum(pred1==pred2$pred1==vowel.test$y)
sum(pred1==pred2$$pred1==vowel.test$y)
sum(pred1==pred2&pred1==vowel.test$y)
sum(pred1==pred2&pred1==vowel.test$y)/sum(pred1==pred2)
library(gbm);set.seed(3433);data(AlzheimerDisease)
adData=data.frame(diagnosis,predictors)
inTrain=createDataPartition(adData$diagnosis, p=.75)[[1]]
training=adData[inTrain,]
testing=adData[-inTrain,]
set.seed(62433);
mod1<-train(diagnosis~.,data=training, method="rf")
mod2<-train(diagnosis~.,data=training, method="gbm",verbose=F)
mod3<-train(diagnosis~.,data=training, method="lda")
pred1<-predict(mod1,testing);pred2<-predict(mod2,testing);pred3<-predict(mod3,testing)
pred<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
comfit<-train(diagnosis~.,method="rf",data=pred)
compred<-predict(comfit,pred)
confusionMatrix(pred1,testing$diagnosis)$overall
confusionMatrix(pred2,testing$diagnosis)$overall
confusionMatrix(pred3,testing$diagnosis)$overall
confusionMatrix(compred,testing$diagnosis)$overall
set.seed(3523)
data(concrete)
inTrain=createDataPartition(concrete$CompressiveStrength, p=.75)[[1]]
training=concrete[inTrain,]
testing=concrete[-inTrain,]
set.seed(233);
fit<-train(CompressiveStrength~., method="lasso", data=training)
fit<-train(CompressiveStrength~., method="lasso", data=training)
plot(fit$finalModel, xvar="penalty", use.color=T)
getwd()
library(lubridate)
install.packages("lubridate")
library(lubridate)
dat = read.csv("~/Desktop/gaData.csv")
getwd()
dat = read.csv("../Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
head(dat)
?bats
install.packages("forecast")
library(forecast)
?bats
t<-bats(tstrain)
f<-forecast(t);accuracy(f,tstrain)
f<-forecast(t)
accuracy(f,testing)
tstest=ts(testing$visitsTumblr)
accuracy(f,tstest)
f<-forecast(t, length(testing$visitsTumblr));
f
head(f)
testing$in<-ifelse(testing$visitsTumblr>f$Lo.95 $ testing$visitsTumblr<f$Hi.95, 1,0)
testing$within<-ifelse(testing$visitsTumblr>f$Lo.95 $ testing$visitsTumblr<f$Hi.95, 1,0)
testing$within<-ifelse((testing$visitsTumblr>f$Lo.95) & (testing$visitsTumblr<f$Hi.95), 1,0)
f$Lo.95
f<-data.frame(f)
testing$within<-ifelse((testing$visitsTumblr>f$Lo.95) & (testing$visitsTumblr<f$Hi.95), 1,0)
head(testing)
table(testing$within)
226/235
sum(testing$within)/length(testing$within)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325);
fit<-svm(CompressiveStrength~., data=training)
pred<-predict(fit,testing)
pred
sqrt(mean((testing$CompressiveStrength-pred)^2))
set.seed(3433);data(AlzheimerDisease)
adData=data.frame(diagnosis,predictors)
inTrain=createDataPartition(adData$diagnosis, p=.75)[[1]]
training=adData[inTrain,]
testing=adData[-inTrain,]
set.seed(62433);
pred1<-predict(mod1,testing);pred2<-predict(mod2,testing);pred3<-predict(mod3,testing)
pred<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
compred<-predict(comfit,testing)
confusionMatrix(pred1,testing$diagnosis)$overall
confusionMatrix(pred2,testing$diagnosis)$overall
confusionMatrix(pred3,testing$diagnosis)$overall
confusionMatrix(compred,testing$diagnosis)$overall
mod1<-train(diagnosis~.,data=training, method="rf")
mod2<-train(diagnosis~.,data=training, method="gbm",verbose=F)
mod3<-train(diagnosis~.,data=training, method="lda")
pred1<-predict(mod1,testing);pred2<-predict(mod2,testing);pred3<-predict(mod3,testing)
pred<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
comfit<-train(diagnosis~.,method="rf",data=pred)
compred<-predict(comfit,testing)
confusionMatrix(pred1,testing$diagnosis)$overall
confusionMatrix(pred2,testing$diagnosis)$overall
confusionMatrix(pred3,testing$diagnosis)$overall
confusionMatrix(compred,testing$diagnosis)$overall
c1<-confusionMatrix(pred1,testing$diagnosis)$overall
c2<-confusionMatrix(pred2,testing$diagnosis)$overall
c3<-confusionMatrix(pred3,testing$diagnosis)$overall
c4<-confusionMatrix(compred,testing$diagnosis)$overall
c1[1]
c(c1[1],c2[1],c3[1],c4[1])
mod1<-train(diagnosis~.,data=training, method="rf",trControl = trainControl(number = 4))
pred1<-predict(mod1,testing);pred2<-predict(mod2,testing);pred3<-predict(mod3,testing)
pred<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
comfit<-train(diagnosis~.,method="rf",data=pred)
compred<-predict(comfit,testing)
c1<-confusionMatrix(pred1,testing$diagnosis)$overall
c2<-confusionMatrix(pred2,testing$diagnosis)$overall
c3<-confusionMatrix(pred3,testing$diagnosis)$overall
c4<-confusionMatrix(compred,testing$diagnosis)$overall
c(c1[1],c2[1],c3[1],c4[1])
library(caret)
library(rattle)
install.packages("rattle")
library(rattle)
1-.9906
install.packages('devtools')
devtools::install_github('rstudio/shinyapps')
library(shinyapps)
library(googleVis)
shiny::runApp('L:/Documents/coursera/data science-Johns Hopkins/9. Developing Data Products')
M<-gvisMotionChart(Fruits,"Fruit","Year", options=list(width=600,height=400))
print(M,"chart")
shinyapps::setAccountInfo(name='lin1212', token='09ED516E1AFDBD9C8E20A10B3DBFF72A', secret='US7D4NGKKdMiWpR0OILIYJraj87TJtojde9Rpu0W')
library(shinyapps)
shinyapps::deployApp('path/to/your/app')
deployApp()
x=c(1,2,3,4);y=c(1,2,3,1)
xp<-c(x,x[1])
xp
yp<-c(y,y[1])
plot(x,y)
lines(xp,yp)
install_github("slidify", "ramnathv")
require(devtools)
install_github("slidify", "ramnathv")
install_github("slidifyLibraries", "ramnathv")
library(slidify)
library(googleVis)
M<-gvisMotionChart(Fruits,"Fruit","Year", options=list(width=600,height=400))
print(M,"chart")
?dTable
library(rCharts)
install.packages("rCharts")
library(rCharts)
install_github('rCharts', 'ramnathv')
library(rCharts)
?dTable
head(airquality)
dTable(airquality,sPaginationType="full_numbers")
?colSums
?colSums
?lm
?show
show
lm
?dgamma
head(dgamma)
getwd()
setwd("l:\Documents/coursera//data science-Johns Hopkins/9. Developing Data Products/")
setwd("l://Documents/coursera//data science-Johns Hopkins//9. Developing Data Products/")
getwd()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
